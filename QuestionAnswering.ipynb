{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd0e852-4ffa-4320-b416-4eb07812a82a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:15:44.376014Z",
     "iopub.status.busy": "2022-07-15T14:15:44.375668Z",
     "iopub.status.idle": "2022-07-15T14:15:54.284595Z",
     "shell.execute_reply": "2022-07-15T14:15:54.283251Z",
     "shell.execute_reply.started": "2022-07-15T14:15:44.375987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.28.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (2.3.2)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.28.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2022.5.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.3)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate\n",
      "  Downloading accelerate-0.10.0-py3-none-any.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.12.0+cu116)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate) (1.23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate) (4.2.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9a6116-746e-4db5-affd-a8dc9efa2702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:10.342846Z",
     "iopub.status.busy": "2022-07-15T14:14:10.342395Z",
     "iopub.status.idle": "2022-07-15T14:14:11.768866Z",
     "shell.execute_reply": "2022-07-15T14:14:11.767507Z",
     "shell.execute_reply.started": "2022-07-15T14:14:10.342771Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertConfig\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "660ba5fe-0b59-49dc-adca-9b6fcbd195de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:11.771748Z",
     "iopub.status.busy": "2022-07-15T14:14:11.771442Z",
     "iopub.status.idle": "2022-07-15T14:14:23.046229Z",
     "shell.execute_reply": "2022-07-15T14:14:23.045354Z",
     "shell.execute_reply.started": "2022-07-15T14:14:11.771743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d9f3f27bd8d45dfbbbba1dbab1ea6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f094d17aeb4121946c4bd44f3b0863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f80237e3952499693a118c5a0200131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a0d72edfc840dfa830f2c5567dd24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c864e7258d44b4bf2d56ae857cf067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1496e7c81574f02a6a107c1d6b51365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ca1f865fce4faa9bd5aadc2bf3acc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90821dea83d40979024ed7e9e7b6a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a001bdadba3e4b1fba90bb4275b797f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c60f387-3042-4047-94f9-9030c09acc92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:23.048057Z",
     "iopub.status.busy": "2022-07-15T14:14:23.047768Z",
     "iopub.status.idle": "2022-07-15T14:14:23.241867Z",
     "shell.execute_reply": "2022-07-15T14:14:23.241011Z",
     "shell.execute_reply.started": "2022-07-15T14:14:23.048033Z"
    }
   },
   "outputs": [],
   "source": [
    "train = Dataset.from_dict(raw_datasets[\"train\"][0:5829])\n",
    "val   = Dataset.from_dict(raw_datasets[\"train\"][43463:44786])\n",
    "test  = Dataset.from_dict(raw_datasets[\"train\"][44786:45428])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7482ae9-dbdb-4a9c-8448-b50a524735f9",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-07-15T14:42:15.723763Z",
     "iopub.status.busy": "2022-07-15T14:42:15.723379Z",
     "iopub.status.idle": "2022-07-15T14:42:18.136177Z",
     "shell.execute_reply": "2022-07-15T14:42:18.135124Z",
     "shell.execute_reply.started": "2022-07-15T14:42:15.723735Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7736a79-a6ce-48da-bed0-d3c8e687fb0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:45.049002Z",
     "iopub.status.busy": "2022-07-15T14:14:45.048618Z",
     "iopub.status.idle": "2022-07-15T14:14:45.065939Z",
     "shell.execute_reply": "2022-07-15T14:14:45.065230Z",
     "shell.execute_reply.started": "2022-07-15T14:14:45.048978Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4c3e13-46b9-4362-accb-dd26b9f1f506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:45.067630Z",
     "iopub.status.busy": "2022-07-15T14:14:45.066789Z",
     "iopub.status.idle": "2022-07-15T14:14:50.042075Z",
     "shell.execute_reply": "2022-07-15T14:14:50.041253Z",
     "shell.execute_reply.started": "2022-07-15T14:14:45.067602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_training_examples at 0x7f0047a26160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5daa2b9ac4a3427c995628276f154898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5829, 5923)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train.column_names,\n",
    ")\n",
    "len(train), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb03fb3d-cc05-49ce-a714-5d97288f9b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:50.044192Z",
     "iopub.status.busy": "2022-07-15T14:14:50.043255Z",
     "iopub.status.idle": "2022-07-15T14:14:55.617654Z",
     "shell.execute_reply": "2022-07-15T14:14:55.616773Z",
     "shell.execute_reply.started": "2022-07-15T14:14:50.044165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706ffda1fd5a4c1292cb77ce442eed2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1323, 1323)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = val.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val.column_names,\n",
    ")\n",
    "len(val), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2826e71b-9c8d-4040-a5e1-1dc2d0da5975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:14:55.619032Z",
     "iopub.status.busy": "2022-07-15T14:14:55.618723Z",
     "iopub.status.idle": "2022-07-15T14:14:58.151358Z",
     "shell.execute_reply": "2022-07-15T14:14:58.150261Z",
     "shell.execute_reply.started": "2022-07-15T14:14:55.619008Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "690a936e-3c17-4399-970a-f8e74039246f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:42:44.134360Z",
     "iopub.status.busy": "2022-07-15T14:42:44.133990Z",
     "iopub.status.idle": "2022-07-15T14:42:44.233669Z",
     "shell.execute_reply": "2022-07-15T14:42:44.232842Z",
     "shell.execute_reply.started": "2022-07-15T14:42:44.134332Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf83b1ad-4cf7-4fc0-9ad3-183e1baa949f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T06:56:20.824810Z",
     "iopub.status.busy": "2022-07-15T06:56:20.824563Z",
     "iopub.status.idle": "2022-07-15T06:56:20.857149Z",
     "shell.execute_reply": "2022-07-15T06:56:20.856416Z",
     "shell.execute_reply.started": "2022-07-15T06:56:20.824784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a30a9e7d0434b03bfc7c162aee7c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8e9f4a0-1898-4ad7-94d6-7e0d5c3f15dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T07:31:29.599665Z",
     "iopub.status.busy": "2022-07-15T07:31:29.598888Z",
     "iopub.status.idle": "2022-07-15T07:31:29.659970Z",
     "shell.execute_reply": "2022-07-15T07:31:29.659065Z",
     "shell.execute_reply.started": "2022-07-15T07:31:29.599638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shashank1303/bert-finetuned-squad-accelerate'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-squad-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38730ce0-8481-4626-ac45-fcd5c88e2357",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T10:08:15.888926Z",
     "iopub.status.busy": "2022-07-15T10:08:15.887885Z",
     "iopub.status.idle": "2022-07-15T10:08:39.566048Z",
     "shell.execute_reply": "2022-07-15T10:08:39.565064Z",
     "shell.execute_reply.started": "2022-07-15T10:08:15.888897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "software-properties-common is already the newest version (0.99.9.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Detected operating system as Ubuntu/focal.\n",
      "Checking for curl...\n",
      "Detected curl...\n",
      "Checking for gpg...\n",
      "Detected gpg...\n",
      "Running apt-get update... done.\n",
      "Installing apt-transport-https... done.\n",
      "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
      "Importing packagecloud gpg key... done.\n",
      "Running apt-get update... done.\n",
      "\n",
      "The repository is setup! You can now install packages.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 38 not upgraded.\n",
      "Need to get 7168 kB of archives.\n",
      "After this operation, 15.6 MB of additional disk space will be used.\n",
      "Get:1 https://packagecloud.io/github/git-lfs/ubuntu focal/main amd64 git-lfs amd64 3.2.0 [7168 kB]\n",
      "Fetched 7168 kB in 1s (7953 kB/s)\n",
      "Selecting previously unselected package git-lfs.\n",
      "(Reading database ... 76197 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_3.2.0_amd64.deb ...\n",
      "Unpacking git-lfs (3.2.0) ...\n",
      "Setting up git-lfs (3.2.0) ...\n",
      "Git LFS initialized.\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install software-properties-common\n",
    "!sudo curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash \n",
    "!sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d6d22eb-bbf0-46cd-abd6-3e8cc0a7935d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T08:57:36.123967Z",
     "iopub.status.busy": "2022-07-15T08:57:36.123127Z",
     "iopub.status.idle": "2022-07-15T08:57:38.318016Z",
     "shell.execute_reply": "2022-07-15T08:57:38.316647Z",
     "shell.execute_reply.started": "2022-07-15T08:57:36.123940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/bert-finetuned-squad-accelerate is already a clone of https://huggingface.co/shashank1303/bert-finetuned-squad-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"bert-finetuned-squad-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbbfe38-c293-4336-8de7-92ce16c9b11b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:21:44.002250Z",
     "iopub.status.busy": "2022-07-15T14:21:44.001668Z",
     "iopub.status.idle": "2022-07-15T14:21:44.012224Z",
     "shell.execute_reply": "2022-07-15T14:21:44.011639Z",
     "shell.execute_reply.started": "2022-07-15T14:21:44.002224Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted_answers = []\n",
    "    metric = load_metric(\"squad\")\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3685d6f6-aebb-410c-8718-c44d13ae5299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T07:58:58.088953Z",
     "iopub.status.busy": "2022-07-15T07:58:58.088391Z",
     "iopub.status.idle": "2022-07-15T07:58:58.093017Z",
     "shell.execute_reply": "2022-07-15T07:58:58.091929Z",
     "shell.execute_reply.started": "2022-07-15T07:58:58.088926Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d14f9804-4167-4809-9695-a3a2c7a6abf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T08:00:03.122793Z",
     "iopub.status.busy": "2022-07-15T08:00:03.121652Z",
     "iopub.status.idle": "2022-07-15T08:00:03.132097Z",
     "shell.execute_reply": "2022-07-15T08:00:03.131428Z",
     "shell.execute_reply.started": "2022-07-15T08:00:03.122764Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForQuestionAnswering(PreTrainedModel):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel.from_pretrained ('bert-base-uncased')\n",
    "        self.qa_outputs = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        token_type_ids = None,\n",
    "        start_positions = None,\n",
    "        end_positions = None\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1).contiguous()\n",
    "        end_logits = end_logits.squeeze(-1).contiguous()\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            # If we are on multi-GPU, split add a dimension\n",
    "            if len(start_positions.size()) > 1:\n",
    "                start_positions = start_positions.squeeze(-1)\n",
    "            if len(end_positions.size()) > 1:\n",
    "                end_positions = end_positions.squeeze(-1)\n",
    "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
    "            ignored_index = start_logits.size(1)\n",
    "            start_positions = start_positions.clamp(0, ignored_index)\n",
    "            end_positions = end_positions.clamp(0, ignored_index)\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "        output = (start_logits, end_logits) + outputs[2:]\n",
    "        return ((total_loss,) + output) if total_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf66674d-a216-444d-9ba1-271cb9f78f54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T08:59:49.333552Z",
     "iopub.status.busy": "2022-07-15T08:59:49.332967Z",
     "iopub.status.idle": "2022-07-15T08:59:51.274431Z",
     "shell.execute_reply": "2022-07-15T08:59:51.272873Z",
     "shell.execute_reply.started": "2022-07-15T08:59:49.333487Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering(BertConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d672f3b-41a4-4bd4-957a-3d02e33a0364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:42:50.943811Z",
     "iopub.status.busy": "2022-07-15T14:42:50.943145Z",
     "iopub.status.idle": "2022-07-15T14:59:31.964470Z",
     "shell.execute_reply": "2022-07-15T14:59:31.963476Z",
     "shell.execute_reply.started": "2022-07-15T14:42:50.943784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7f871f1e4b480a83dd276c487957f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/741 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 iteration: 0 training loss: 6.010009765625\n",
      "epoch: 0 iteration: 100 training loss: 4.470323014967512\n",
      "epoch: 0 iteration: 200 training loss: 3.5704570789242265\n",
      "epoch: 0 iteration: 300 training loss: 3.0654898988844153\n",
      "epoch: 0 iteration: 400 training loss: 2.7716740634375974\n",
      "epoch: 0 iteration: 500 training loss: 2.5575946286290945\n",
      "epoch: 0 iteration: 600 training loss: 2.399202914880635\n",
      "epoch: 0 iteration: 700 training loss: 2.2754977426243235\n",
      "Evaluation!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7609d7b0bfb4b009a1a6b6aa2e1843c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b8e1b086654a14a5989e0dd7f5e8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: , train loss 2.2413433511891023 {'exact_match': 51.70068027210884, 'f1': 66.73202779349897}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_avg_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        #print(batch)\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        train_avg_loss += loss.item()\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"epoch: {epoch} iteration: {step} training loss: {train_avg_loss/(step+1)}\")\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs[0]).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs[1]).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, val\n",
    "    )\n",
    "    print(f\"epoch {epoch}: , train loss {train_avg_loss/len(train_dataloader)}\", metrics)\n",
    "\n",
    "    # Save and upload\n",
    "    \"\"\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c963a843-a29d-4208-ab9b-86a625439cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:59:54.933804Z",
     "iopub.status.busy": "2022-07-15T14:59:54.933451Z",
     "iopub.status.idle": "2022-07-15T14:59:57.783715Z",
     "shell.execute_reply": "2022-07-15T14:59:57.782980Z",
     "shell.execute_reply.started": "2022-07-15T14:59:54.933780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dc76f6353d4cacb1e39bf99164112c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = test.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=test.column_names,\n",
    ")\n",
    "test_set = test_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_set.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, collate_fn=default_data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataloader = accelerator.prepare(\n",
    "    test_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "596b069a-fe9b-47e5-af1b-cc2b503cebf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:59:57.785365Z",
     "iopub.status.busy": "2022-07-15T14:59:57.785088Z",
     "iopub.status.idle": "2022-07-15T15:00:33.254720Z",
     "shell.execute_reply": "2022-07-15T15:00:33.253735Z",
     "shell.execute_reply.started": "2022-07-15T14:59:57.785340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb5af612f24741b746504bef9889ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47681e9631be47cbbb6deae9c1bd7f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 48.13084112149533, 'f1': 64.001094077288}\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "start_logits = []\n",
    "end_logits = []\n",
    "print(\"Test!\")\n",
    "for batch in tqdm(test_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "     \n",
    "    start_logits.append(outputs[0].cpu().numpy())\n",
    "    end_logits.append(outputs[0].cpu().numpy())\n",
    "\n",
    "start_logits = np.concatenate(start_logits)\n",
    "end_logits = np.concatenate(end_logits)\n",
    "start_logits = start_logits[: len(validation_dataset)]\n",
    "end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "metrics = compute_metrics(\n",
    "    start_logits, end_logits, test_dataset, test\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5212dbd-73b9-4b24-b7b4-5ad7c2fdc00a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-11T09:13:24.175595Z",
     "iopub.status.busy": "2022-07-11T09:13:24.175103Z",
     "iopub.status.idle": "2022-07-11T09:13:27.354693Z",
     "shell.execute_reply": "2022-07-11T09:13:27.353255Z",
     "shell.execute_reply.started": "2022-07-11T09:13:24.175550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9303467273712158, 'start': 136, 'end': 166, 'answer': 'Abhishek, Shashank and Dheeraj'}\n",
      "{'score': 0.9721232652664185, 'start': 26, 'end': 44, 'answer': 'sir Chandra Sekhar'}\n",
      "{'score': 0.1526615172624588, 'start': 223, 'end': 226, 'answer': 'sir'}\n",
      "{'score': 0.12367977201938629, 'start': 95, 'end': 113, 'answer': 'question answering'}\n",
      "{'score': 0.5629004240036011, 'start': 77, 'end': 113, 'answer': 'finetune BERT for question answering'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"shashank1303/bert-finetuned-squad-accelerate\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "The course is thaught by sir Chandra Sekhar and one of the assignment is to finetune BERT for question answering. \n",
    "My team members are Abhishek, Shashank and Dheeraj.We need to give viva sometime next week and is taken by sir.\n",
    "\"\"\"\n",
    "questions = [\"Names of my team members?\",\"Who is teaching the course?\",\"Who is taking is viva?\",\"What is the task given to students?\",\n",
    "            \"What is the assignment?\"]\n",
    "for question in questions:\n",
    "    print(question_answerer(question=question, context=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97aa73-be1f-4803-92e6-64fbc6d99844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
